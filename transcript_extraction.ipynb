{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08dcf386-768d-40cf-ac7b-36e6f7a170fa",
   "metadata": {},
   "source": [
    "TODO\n",
    "* Merge Tables 2,4,10 to work together\n",
    "* Create loop for full season\n",
    "* Add season and episode to front of data frame\n",
    "\n",
    "Someday:\n",
    "* Fix Italics:\n",
    "    * Work: \n",
    "        * italic_text_with_suffix =  '12345' + italics.get_text(strip=True) + '12345'\n",
    "    * Not work: \n",
    "        * italic_text_with_suffix =  '12345' + italics.get_text(strip=True) + '12345'\n",
    "        \n",
    "Learning: \n",
    "* Look for specific class types in BeautifulSoup. The fact that the transcript was formated as a *wikitable* made life much easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d5bc47f-88c2-433e-80d5-4c9fb8e177ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "#standard\n",
    "import pandas as pd\n",
    "#import random\n",
    "from datetime import datetime\n",
    "from pandasql import sqldf\n",
    "pysqldf = lambda q: sqldf(q,globals())\n",
    "\n",
    "#Viz\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "#Data Pulls\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d917c88-75c3-40e2-b272-cc0b5144f964",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Episode titles for Season 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62c837a8-d98c-42dc-abf5-6b87fb049837",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_base = 'https://avatar.fandom.com/wiki/Avatar_Wiki:Transcripts'\n",
    "response = requests.get(url_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14407dfe-7e36-491b-9650-860c68962901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 90 tables on the page.\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "tables = soup.find_all(\"table\")\n",
    "\n",
    "print(f\"Found {len(tables)} tables on the page.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "127de1c4-c26a-4842-b650-dfbdb26670f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 2: Book 1 Water\n",
    "# Table 4: Book 2 Earth\n",
    "# Table 10: Book 3 Fire\n",
    "\n",
    "# if len(tables) > 0:\n",
    "#     print(\"First table:\")\n",
    "#     print(tables[10].prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35332e71-cbbe-4f27-8944-597666883618",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_list = [2,5,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c972c08f-fce0-42e9-88d7-8f80970e9e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_list = []\n",
    "\n",
    "for book in book_list:\n",
    "    specific_table = soup.find_all(\"table\")[book]  # Replace 0 with the index of the table you want\n",
    "    links = specific_table.find_all(\"a\")\n",
    "    for link in links:\n",
    "        episode_list.append(link.get(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ecb95ae-fb85-4093-b529-977061278d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_list_filtered = [episode for episode in episode_list if \"commentary\" not in episode]\n",
    "#episode_list_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69d03d8c-8613-4f9b-9c4e-7bbb15b49130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://avatar.fandom.com/wiki/Transcript:The_Boy_in_the_Iceberg\n",
      "https://avatar.fandom.com/wiki/Transcript:The_Avatar_Returns\n",
      "https://avatar.fandom.com/wiki/Transcript:The_Southern_Air_Temple\n",
      "https://avatar.fandom.com/wiki/Transcript:The_Warriors_of_Kyoshi\n",
      "https://avatar.fandom.com/wiki/Transcript:The_King_of_Omashu\n",
      "https://avatar.fandom.com/wiki/Transcript:Imprisoned\n",
      "https://avatar.fandom.com/wiki/Transcript:Winter_Solstice,_Part_1:_The_Spirit_World\n",
      "https://avatar.fandom.com/wiki/Transcript:Winter_Solstice,_Part_2:_Avatar_Roku\n",
      "https://avatar.fandom.com/wiki/Transcript:The_Waterbending_Scroll\n",
      "https://avatar.fandom.com/wiki/Transcript:Jet_(episode)\n",
      "https://avatar.fandom.com/wiki/Transcript:The_Great_Divide\n",
      "https://avatar.fandom.com/wiki/Transcript:The_Storm\n",
      "https://avatar.fandom.com/wiki/Transcript:The_Blue_Spirit\n",
      "https://avatar.fandom.com/wiki/Transcript:The_Fortuneteller\n",
      "https://avatar.fandom.com/wiki/Transcript:Bato_of_the_Water_Tribe\n",
      "https://avatar.fandom.com/wiki/Transcript:The_Deserter\n",
      "https://avatar.fandom.com/wiki/Transcript:The_Northern_Air_Temple\n",
      "https://avatar.fandom.com/wiki/Transcript:The_Waterbending_Master\n",
      "https://avatar.fandom.com/wiki/Transcript:The_Siege_of_the_North,_Part_1\n",
      "https://avatar.fandom.com/wiki/Transcript:The_Siege_of_the_North,_Part_2\n",
      "https://avatar.fandom.com/wiki/Transcript:The_Avatar_State\n",
      "https://avatar.fandom.com/wiki/Transcript:The_Cave_of_Two_Lovers\n",
      "https://avatar.fandom.com/wiki/Transcript:Return_to_Omashu\n",
      "https://avatar.fandom.com/wiki/Transcript:The_Swamp\n",
      "https://avatar.fandom.com/wiki/Transcript:Avatar_Day\n",
      "https://avatar.fandom.com/wiki/Transcript:The_Blind_Bandit\n",
      "https://avatar.fandom.com/wiki/Transcript:Zuko_Alone\n",
      "https://avatar.fandom.com/wiki/Transcript:The_Chase\n",
      "https://avatar.fandom.com/wiki/Transcript:Bitter_Work\n",
      "https://avatar.fandom.com/wiki/Transcript:The_Library\n",
      "https://avatar.fandom.com/wiki/Transcript:The_Desert\n",
      "https://avatar.fandom.com/wiki/Transcript:The_Serpent%27s_Pass\n",
      "https://avatar.fandom.com/wiki/Transcript:The_Drill\n",
      "https://avatar.fandom.com/wiki/Transcript:City_of_Walls_and_Secrets\n",
      "https://avatar.fandom.com/wiki/Transcript:The_Tales_of_Ba_Sing_Se\n",
      "https://avatar.fandom.com/wiki/Transcript:Appa%27s_Lost_Days\n",
      "https://avatar.fandom.com/wiki/Transcript:Lake_Laogai_(episode)\n",
      "https://avatar.fandom.com/wiki/Transcript:The_Earth_King\n",
      "https://avatar.fandom.com/wiki/Transcript:The_Guru\n",
      "https://avatar.fandom.com/wiki/Transcript:The_Crossroads_of_Destiny\n",
      "https://avatar.fandom.com/wiki/Transcript:The_Awakening\n",
      "https://avatar.fandom.com/wiki/Transcript:The_Headband\n",
      "https://avatar.fandom.com/wiki/Transcript:The_Painted_Lady\n",
      "https://avatar.fandom.com/wiki/Transcript:Sokka%27s_Master\n",
      "https://avatar.fandom.com/wiki/Transcript:The_Beach\n",
      "https://avatar.fandom.com/wiki/Transcript:The_Avatar_and_the_Fire_Lord\n",
      "https://avatar.fandom.com/wiki/Transcript:The_Runaway\n",
      "https://avatar.fandom.com/wiki/Transcript:The_Puppetmaster\n",
      "https://avatar.fandom.com/wiki/Transcript:Nightmares_and_Daydreams\n",
      "https://avatar.fandom.com/wiki/Transcript:The_Day_of_Black_Sun,_Part_1:_The_Invasion\n",
      "https://avatar.fandom.com/wiki/Transcript:The_Day_of_Black_Sun,_Part_2:_The_Eclipse\n",
      "https://avatar.fandom.com/wiki/Transcript:The_Western_Air_Temple\n",
      "https://avatar.fandom.com/wiki/Transcript:The_Firebending_Masters\n",
      "https://avatar.fandom.com/wiki/Transcript:The_Boiling_Rock,_Part_1\n",
      "https://avatar.fandom.com/wiki/Transcript:The_Boiling_Rock,_Part_2\n",
      "https://avatar.fandom.com/wiki/Transcript:The_Southern_Raiders\n",
      "https://avatar.fandom.com/wiki/Transcript:The_Ember_Island_Players\n",
      "https://avatar.fandom.com/wiki/Transcript:Sozin%27s_Comet,_Part_1:_The_Phoenix_King\n",
      "https://avatar.fandom.com/wiki/Transcript:Sozin%27s_Comet,_Part_2:_The_Old_Masters\n",
      "https://avatar.fandom.com/wiki/Transcript:Sozin%27s_Comet,_Part_3:_Into_the_Inferno\n",
      "https://avatar.fandom.com/wiki/Transcript:Sozin%27s_Comet,_Part_4:_Avatar_Aang\n"
     ]
    }
   ],
   "source": [
    "#Creates Episodes URLs\n",
    "base = 'https://avatar.fandom.com'\n",
    "url_episode_list = []\n",
    "for episode in episode_list_filtered:\n",
    "    print(base + episode)\n",
    "    url_episode_list.append(base + episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e82b324-2693-4fcd-af6e-94bb6d8013f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Edge Case Identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "374d4c67-b5ba-469c-b74b-a9bf93acfd32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "The_Boy_in_the_Iceberg\n",
      "https://avatar.fandom.com/wiki/Transcript:The_Boy_in_the_Iceberg\n",
      "Number of Tables: 2\n",
      "2\n",
      "The_Avatar_Returns\n",
      "https://avatar.fandom.com/wiki/Transcript:The_Avatar_Returns\n",
      "Number of Tables: 2\n",
      "15\n",
      "Bato_of_the_Water_Tribe\n",
      "https://avatar.fandom.com/wiki/Transcript:Bato_of_the_Water_Tribe\n",
      "Number of Tables: 2\n",
      "35\n",
      "The_Tales_of_Ba_Sing_Se\n",
      "https://avatar.fandom.com/wiki/Transcript:The_Tales_of_Ba_Sing_Se\n",
      "Number of Tables: 6\n",
      "48\n",
      "The_Puppetmaster\n",
      "https://avatar.fandom.com/wiki/Transcript:The_Puppetmaster\n",
      "Number of Tables: 2\n"
     ]
    }
   ],
   "source": [
    "#Printe Edge Case Episodes\n",
    "for episode_number, url in enumerate(url_episode_list):\n",
    "    \n",
    "    split_text = url.split(':', 2)\n",
    "    episode_title = split_text[2]\n",
    "\n",
    "    episode_number += 1\n",
    "    # print(episode_number)\n",
    "    # print(episode_title)\n",
    "    # print(url)\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    # Parse the page with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all the tables with the class \"wikitable\"\n",
    "    wikitable_tables = soup.find_all(\"table\", class_=\"wikitable\")\n",
    "    num_tables = len(wikitable_tables)\n",
    "    \n",
    "    if num_tables >= 2:\n",
    "            print(episode_number)\n",
    "            print(episode_title)\n",
    "            print(url)\n",
    "            print(f\"Number of Tables: {len(wikitable_tables)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d591555f-8f5a-4093-9f4c-11ec364d57fc",
   "metadata": {},
   "source": [
    "# Edge Case Exploration\n",
    "\n",
    "#url = 'https://avatar.fandom.com/wiki/Transcript:The_Boy_in_the_Iceberg'\n",
    "# Index 1 | 2nd Table\n",
    "\n",
    "#url = 'https://avatar.fandom.com/wiki/Transcript:The_Avatar_Returns'\n",
    "# Index 1 2nd Table \n",
    "\n",
    "# url = 'https://avatar.fandom.com/wiki/Transcript:Bato_of_the_Water_Tribe'\n",
    "# Index 0 1st Table \n",
    "# 2nd Table contains a deleted scene\n",
    "\n",
    "#url = 'https://avatar.fandom.com/wiki/Transcript:The_Tales_of_Ba_Sing_Se'\n",
    "# Index 0 The Tale of Toph and Katara\n",
    "# Index 1 The Tale of Iroh\n",
    "# Index 2 The Tale of Aang\n",
    "# Index 3 The Tale of Sokka \n",
    "# Index 4 THe Tale of Zuko\n",
    "\n",
    "#url = 'https://avatar.fandom.com/wiki/Transcript:The_Puppetmaster'\n",
    "# Index 1 2nd Table \n",
    "# Removes the \"previously on Avatar\"\n",
    "    \n",
    "response = requests.get(url)\n",
    "ep_soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "tables = ep_soup.select(\"table.wikitable\")\n",
    "\n",
    "#print(tables)\n",
    "\n",
    "data = []\n",
    "\n",
    "print(f\"Number of Tables: {len(tables)}\")\n",
    "print()\n",
    "\n",
    "desired_table = tables[:4]  \n",
    "# Find all rows within the 2nd Table that holds the transcript\n",
    "rows = desired_table.select(\"tbody tr\")\n",
    "\n",
    "for row in rows:\n",
    "    character_cell = row.find('th')  # Find the <th> for character names\n",
    "    dialogue_cell = row.find('td')    # Find the <td> for character's dialogue\n",
    "\n",
    "    if character_cell and dialogue_cell:  # Ensure both character & dialogue are present (thus skipping pure screen directions)\n",
    "        character = character_cell.get_text(strip=True)\n",
    "\n",
    "        # Replace <i> tags with their text (ITALICS CAUSING PROBLEMS)\n",
    "        for italics in dialogue_cell.find_all('i'):\n",
    "            # Get the text inside <i>, append '123' and replace the <i> with this text\n",
    "            italic_text_with_suffix =  '12345' + italics.get_text(strip=True) + '12345'\n",
    "            italics.replace_with(italic_text_with_suffix)  # Replace <i> tag with modified text\n",
    "\n",
    "        dialogue = dialogue_cell.get_text(strip=True)  # Get the text after modifying italics\n",
    "\n",
    "        data.append({'character': character, 'dialogue': dialogue})\n",
    "\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "#Cleaning\n",
    "df['dialogue'] = df['dialogue'].str.replace(r'\\[.*?\\]', '', regex=True).str.strip()\n",
    "df['dialogue'] = df['dialogue'].str.replace(r'\\(.*?\\)', '', regex=True).str.strip()\n",
    "df['dialogue'] = df['dialogue'].str.replace('12345', ' ').str.strip()\n",
    "\n",
    "#Word Count\n",
    "df['word_count'] = df['dialogue'].str.split().apply(len)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68176747-53ac-4ecc-b51c-cdec0688306d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tales of Ba Sing Se - Edge Case Exploration\n",
    "url = 'https://avatar.fandom.com/wiki/Transcript:The_Tales_of_Ba_Sing_Se'\n",
    "# Index 0 The Tale of Toph and Katara\n",
    "# Index 1 The Tale of Iroh\n",
    "# Index 2 The Tale of Aang\n",
    "# Index 3 The Tale of Sokka \n",
    "# Index 4 THe Tale of Zuko\n",
    "    \n",
    "response = requests.get(url)\n",
    "ep_soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "tables = ep_soup.select(\"table.wikitable\")\n",
    "\n",
    "#print(tables)\n",
    "\n",
    "data = []\n",
    "\n",
    "# Iterate over the first 5 tables (0 to 4)\n",
    "for table in tables[:5]:  \n",
    "    # Find all rows within each table\n",
    "    rows = table.select(\"tbody tr\")\n",
    "\n",
    "    for row in rows:\n",
    "        character_cell = row.find('th')  # Find the <th> for character names\n",
    "        dialogue_cell = row.find('td')   # Find the <td> for character's dialogue\n",
    "\n",
    "        if character_cell and dialogue_cell:  # Ensure both character & dialogue are present\n",
    "            character = character_cell.get_text(strip=True)\n",
    "\n",
    "            # Process the <i> tags within the dialogue\n",
    "            for italics in dialogue_cell.find_all('i'):\n",
    "                # Prepend and append '12345' to italicized text\n",
    "                italic_text_with_suffix = '12345' + italics.get_text(strip=True) + '12345'\n",
    "                italics.replace_with(italic_text_with_suffix)  # Replace <i> tag with modified text\n",
    "\n",
    "            # Get the full dialogue text after modifications\n",
    "            dialogue = dialogue_cell.get_text(strip=True)\n",
    "\n",
    "            # Append the data to the list\n",
    "            data.append({'character': character, 'dialogue': dialogue})\n",
    "\n",
    "# Convert the collected data into a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "#Cleaning\n",
    "df['dialogue'] = df['dialogue'].str.replace(r'\\[.*?\\]', '', regex=True).str.strip()\n",
    "df['dialogue'] = df['dialogue'].str.replace(r'\\(.*?\\)', '', regex=True).str.strip()\n",
    "df['dialogue'] = df['dialogue'].str.replace('12345', ' ').str.strip()\n",
    "\n",
    "#Word Count\n",
    "df['word_count'] = df['dialogue'].str.split().apply(len)\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62f32b5-3780-4316-a893-312443f6759a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "#pd.reset_option('display.max_rows')\n",
    "df.head(195)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14e2340-5a0a-48aa-95f8-be4d2e29375c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Non Edge Case Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2948e5ae-2f61-4243-b1e2-8659ef5c6616",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IDentify problem of if intro is said"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101b3912-64b7-4f48-9120-73bf40876aa0",
   "metadata": {},
   "source": [
    "### Full Season Edge Case + Non Edge Case (...except Tales of Ba Sing Se) Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1441b3a-7b65-4216-80fa-81762e2a10ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#episodes, so technically need to subtract 1 from each to get the edge case\n",
    "edge_case_episodes = [1, #The_Boy_in_the_Iceberg\n",
    "                      2, #The_Avatar_Returns\n",
    "                      #15, #Bato_of_the_Water_Tribe - Use table 1, index 0 to remove deleted scence\n",
    "                      35, #The_Tales_of_Ba_Sing_Se\n",
    "                      48 #The_Puppetmaster\n",
    "                     ]\n",
    "\n",
    "#Subtract 1 from each episode to convert episode_number --> index\n",
    "edge_case_episodes = [episode - 1 for episode in edge_case_episodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84b5c0bb-2fe6-4862-b06a-72be2d59b7c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 34, 47]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_case_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbca0fb-05fc-47ca-9e31-d514c9fde372",
   "metadata": {},
   "outputs": [],
   "source": [
    "### WORKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78dd7083-a913-4c58-aab2-4d239b74fa4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge Case\n",
      "0\n",
      "Sozin%27s_Comet,_Part_4:_Avatar_Aang\n",
      "https://avatar.fandom.com/wiki/Transcript:The_Boy_in_the_Iceberg\n",
      "\n",
      "Edge Case\n",
      "1\n",
      "The_Boy_in_the_Iceberg\n",
      "https://avatar.fandom.com/wiki/Transcript:The_Avatar_Returns\n",
      "\n",
      "34\n",
      "City_of_Walls_and_Secrets\n",
      "https://avatar.fandom.com/wiki/Transcript:The_Tales_of_Ba_Sing_Se\n",
      "\n",
      "Ba Sing Se\n",
      "Edge Case\n",
      "47\n",
      "The_Runaway\n",
      "https://avatar.fandom.com/wiki/Transcript:The_Puppetmaster\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Catches all edge cases excepct Tales of Ba Sing Se\n",
    "# Can't get it to work.. .\n",
    "\n",
    "transcript_df = pd.DataFrame()\n",
    "\n",
    "# Loop through each episode\n",
    "# Can expand to more episodes\n",
    "for episode_number, url in enumerate(url_episode_list):\n",
    "    # Splits based on Edge Cases\n",
    "    \n",
    "    data = []  # Initialize data list for the current episode\n",
    "    \n",
    "    #Use if statement to get stuff... \n",
    "    if episode_number not in edge_case_episodes:\n",
    "        \n",
    "        response = requests.get(url)\n",
    "        ep_soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        tables = ep_soup.select(\"table.wikitable\")\n",
    "        #print(\"Non Edge Case\")\n",
    "        \n",
    "        desired_table = tables[0]\n",
    "        rows = desired_table.select(\"tbody tr\")\n",
    "\n",
    "    if episode_number in edge_case_episodes:\n",
    "            \n",
    "            if episode_number != 34:\n",
    "                response = requests.get(url)\n",
    "                ep_soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                tables = ep_soup.select(\"table.wikitable\")\n",
    "                \n",
    "                desired_table = tables[1]\n",
    "                rows = desired_table.select(\"tbody tr\")\n",
    "                \n",
    "                print(\"Edge Case\")\n",
    "                print(episode_number)\n",
    "                print(episode_title)\n",
    "                print(url)\n",
    "                print()\n",
    "                \n",
    "            else:\n",
    "                response = requests.get(url)\n",
    "                ep_soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                tables = ep_soup.select(\"table.wikitable\")\n",
    "\n",
    "                print(episode_number)\n",
    "                print(episode_title)\n",
    "                print(url)\n",
    "                print()\n",
    "\n",
    "                print('Ba Sing Se')\n",
    "\n",
    "                all_rows = []  # Initialize list to store all rows from all tables\n",
    "\n",
    "                for table in tables[:5]:\n",
    "                    table_rows = table.select(\"tbody tr\")\n",
    "                    all_rows.extend(table_rows)  # Append all rows to the all_rows list\n",
    "\n",
    "                rows = all_rows  # Now, 'rows' contains all the rows from tables 0 to 4\n",
    "                    \n",
    "        \n",
    "    split_text = url.split(':', 2)\n",
    "    episode_title = split_text[2]\n",
    "    episode_number += 1\n",
    "    \n",
    "    #print(episode_number)\n",
    "    #print(episode_title)\n",
    "    #print(url)\n",
    "    #print()\n",
    "\n",
    "    for row in rows:\n",
    "        character_cell = row.find('th')  # Find the <th> for character names\n",
    "        dialogue_cell = row.find('td')   # Find the <td> for character's dialogue\n",
    "\n",
    "        if character_cell and dialogue_cell:  # Ensure both character & dialogue are present\n",
    "            character = character_cell.get_text(strip=True)\n",
    "\n",
    "            # Replace <i> tags with their text (handle italicized text)\n",
    "            for italics in dialogue_cell.find_all('i'):\n",
    "                italic_text_with_suffix = '12345' + italics.get_text(strip=True) + '12345'\n",
    "                italics.replace_with(italic_text_with_suffix)  # Replace <i> tag with modified text\n",
    "\n",
    "            dialogue = dialogue_cell.get_text(strip=True)  # Get the text after modifying italics\n",
    "\n",
    "            # Append data for this row\n",
    "            data.append({'character': character, 'dialogue': dialogue})\n",
    "\n",
    "    # Only create DataFrame if data was extracted        \n",
    "    if data:\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        # Cleaning the dialogue column\n",
    "        df['dialogue'] = df['dialogue'].str.replace(r'\\[.*?\\]', '', regex=True).str.strip()  # Remove text in []\n",
    "        df['dialogue'] = df['dialogue'].str.replace(r'\\(.*?\\)', '', regex=True).str.strip()  # Remove text in ()\n",
    "        df['dialogue'] = df['dialogue'].str.replace('12345', ' ').str.strip()  # Replace '12345' with space\n",
    "\n",
    "        # Add word count\n",
    "        df['word_count'] = df['dialogue'].str.split().apply(len)\n",
    "\n",
    "        # Add episode number and title\n",
    "        df['episode'] = episode_number\n",
    "        df['episode_title'] = episode_title  # Fix the typo\n",
    "\n",
    "        # Concatenate with the main transcript DataFrame\n",
    "        transcript_df = pd.concat([transcript_df, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bfc8a24d-2465-4037-b5c3-b196a1630d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>character</th>\n",
       "      <th>dialogue</th>\n",
       "      <th>word_count</th>\n",
       "      <th>episode</th>\n",
       "      <th>episode_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5827</th>\n",
       "      <td>Katara</td>\n",
       "      <td>Toph! Aren't you gonna get ready for the day?</td>\n",
       "      <td>9</td>\n",
       "      <td>35</td>\n",
       "      <td>The_Tales_of_Ba_Sing_Se</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5828</th>\n",
       "      <td>Toph</td>\n",
       "      <td>I'm ready.</td>\n",
       "      <td>2</td>\n",
       "      <td>35</td>\n",
       "      <td>The_Tales_of_Ba_Sing_Se</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5829</th>\n",
       "      <td>Katara</td>\n",
       "      <td>You're not gonna wash up? You've got a little ...</td>\n",
       "      <td>15</td>\n",
       "      <td>35</td>\n",
       "      <td>The_Tales_of_Ba_Sing_Se</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5830</th>\n",
       "      <td>Toph</td>\n",
       "      <td>You call it dirt, I call it a healthy coating ...</td>\n",
       "      <td>12</td>\n",
       "      <td>35</td>\n",
       "      <td>The_Tales_of_Ba_Sing_Se</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5831</th>\n",
       "      <td>Katara</td>\n",
       "      <td>Hmm ...  You know what we need? A girl's day out!</td>\n",
       "      <td>11</td>\n",
       "      <td>35</td>\n",
       "      <td>The_Tales_of_Ba_Sing_Se</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5948</th>\n",
       "      <td>Jin</td>\n",
       "      <td>Your uncle is a good teacher.  I have somethin...</td>\n",
       "      <td>20</td>\n",
       "      <td>35</td>\n",
       "      <td>The_Tales_of_Ba_Sing_Se</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5949</th>\n",
       "      <td>Jin</td>\n",
       "      <td>What's wrong?</td>\n",
       "      <td>2</td>\n",
       "      <td>35</td>\n",
       "      <td>The_Tales_of_Ba_Sing_Se</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5950</th>\n",
       "      <td>Zuko</td>\n",
       "      <td>It's complicated. I have to go.</td>\n",
       "      <td>6</td>\n",
       "      <td>35</td>\n",
       "      <td>The_Tales_of_Ba_Sing_Se</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5951</th>\n",
       "      <td>Iroh</td>\n",
       "      <td>How was your night, Prince Zuko?</td>\n",
       "      <td>6</td>\n",
       "      <td>35</td>\n",
       "      <td>The_Tales_of_Ba_Sing_Se</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5952</th>\n",
       "      <td>Zuko</td>\n",
       "      <td>It was nice.</td>\n",
       "      <td>3</td>\n",
       "      <td>35</td>\n",
       "      <td>The_Tales_of_Ba_Sing_Se</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>126 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     character                                           dialogue  word_count  \\\n",
       "5827    Katara      Toph! Aren't you gonna get ready for the day?           9   \n",
       "5828      Toph                                         I'm ready.           2   \n",
       "5829    Katara  You're not gonna wash up? You've got a little ...          15   \n",
       "5830      Toph  You call it dirt, I call it a healthy coating ...          12   \n",
       "5831    Katara  Hmm ...  You know what we need? A girl's day out!          11   \n",
       "...        ...                                                ...         ...   \n",
       "5948       Jin  Your uncle is a good teacher.  I have somethin...          20   \n",
       "5949       Jin                                      What's wrong?           2   \n",
       "5950      Zuko                    It's complicated. I have to go.           6   \n",
       "5951      Iroh                   How was your night, Prince Zuko?           6   \n",
       "5952      Zuko                                       It was nice.           3   \n",
       "\n",
       "      episode            episode_title  \n",
       "5827       35  The_Tales_of_Ba_Sing_Se  \n",
       "5828       35  The_Tales_of_Ba_Sing_Se  \n",
       "5829       35  The_Tales_of_Ba_Sing_Se  \n",
       "5830       35  The_Tales_of_Ba_Sing_Se  \n",
       "5831       35  The_Tales_of_Ba_Sing_Se  \n",
       "...       ...                      ...  \n",
       "5948       35  The_Tales_of_Ba_Sing_Se  \n",
       "5949       35  The_Tales_of_Ba_Sing_Se  \n",
       "5950       35  The_Tales_of_Ba_Sing_Se  \n",
       "5951       35  The_Tales_of_Ba_Sing_Se  \n",
       "5952       35  The_Tales_of_Ba_Sing_Se  \n",
       "\n",
       "[126 rows x 5 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript_df[transcript_df['episode'] ==35].head(126)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2532830e-7fa3-4327-8b54-323df3b03c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed25336-2151-45ca-9855-e82ba8e129fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed0c743-e1a2-4acd-a134-5de17bb92d28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ab87e8-e2ff-49a6-90b2-f4931ac8fa4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae507759-f6ed-4c20-84a1-fce40776679f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006c26b1-6feb-439f-87b7-8279239fddc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "###TEST 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c538c4c6-eff9-4918-9541-159ec69e5b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dialogue(dialogue_cell):\n",
    "    for italics in dialogue_cell.find_all('i'):\n",
    "        italic_text_with_suffix = '12345' + italics.get_text(strip=True) + '12345'\n",
    "        italics.replace_with(italic_text_with_suffix)\n",
    "    return dialogue_cell.get_text(strip=True)\n",
    "\n",
    "def clean_dataframe(df, episode_number, episode_title):\n",
    "    df['dialogue'] = df['dialogue'].str.replace(r'\\[.*?\\]', '', regex=True).str.strip()\n",
    "    df['dialogue'] = df['dialogue'].str.replace(r'\\(.*?\\)', '', regex=True).str.strip()\n",
    "    df['dialogue'] = df['dialogue'].str.replace('12345', ' ').str.strip()\n",
    "    df['word_count'] = df['dialogue'].str.split().apply(len)\n",
    "    df['episode'] = episode_number\n",
    "    df['episode_title'] = episode_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caef54ba-856a-440c-a78b-717ecd7f968c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode_number, url in enumerate(url_episode_list):\n",
    "    data = []  # Initialize data list for the current episode\n",
    "\n",
    "    response = requests.get(url)\n",
    "    ep_soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    tables = ep_soup.select(\"table.wikitable\")\n",
    "\n",
    "    # Split episode title\n",
    "    split_text = url.split(':', 2)\n",
    "    episode_title = split_text[2]\n",
    "    episode_number += 1\n",
    "\n",
    "    # Check if it's a non-edge case episode\n",
    "    if episode_number not in edge_case_episodes:\n",
    "        print(\"Non Edge Case\")\n",
    "        desired_table = tables[0]\n",
    "        rows = desired_table.select(\"tbody tr\")\n",
    "\n",
    "    # Handle edge case episodes\n",
    "    if episode_number in edge_case_episodes:\n",
    "        print(\"Edge Case\")\n",
    "        \n",
    "        # Special case for episode 35 (Ba Sing Se)\n",
    "        if episode_number == 34:\n",
    "            print('Ba Sing Se')\n",
    "            for table in tables[:5]:  # Loop through tables 0 to 4\n",
    "                rows = table.select(\"tbody tr\")\n",
    "                \n",
    "                # Process rows from each table\n",
    "                for row in rows:\n",
    "                    character_cell = row.find('th')\n",
    "                    dialogue_cell = row.find('td')\n",
    "                    if character_cell and dialogue_cell:\n",
    "                        character = character_cell.get_text(strip=True)\n",
    "                        dialogue = process_dialogue(dialogue_cell)  # Extract and clean dialogue\n",
    "                        data.append({'character': character, 'dialogue': dialogue})\n",
    "        else:\n",
    "            print(f\"Processing Edge Case Episode {episode_number}\")\n",
    "            desired_table = tables[1]  # Only select the second table for other edge case episodes\n",
    "            rows = desired_table.select(\"tbody tr\")\n",
    "\n",
    "    # Common row processing (both edge case and non-edge case)\n",
    "    for row in rows:\n",
    "        character_cell = row.find('th')\n",
    "        dialogue_cell = row.find('td')\n",
    "\n",
    "        if character_cell and dialogue_cell:\n",
    "            character = character_cell.get_text(strip=True)\n",
    "            dialogue = process_dialogue(dialogue_cell)\n",
    "            data.append({'character': character, 'dialogue': dialogue})\n",
    "\n",
    "    # Only create DataFrame if data was extracted\n",
    "    if data:\n",
    "        df = pd.DataFrame(data)\n",
    "        clean_dataframe(df, episode_number, episode_title)\n",
    "        transcript_df = pd.concat([transcript_df, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c4188b-09d3-41dd-89e4-72bd121aaab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_df[transcript_df['episode'] ==35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51b0130-12fe-4c03-bf28-6aaa844c85bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030636c9-42b1-4c96-9b34-5e2a4063ec4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584ea13c-6eaf-49ec-ad14-d93f2f8ad572",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bbc81e-e2f9-41bc-babf-1766d17c3402",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Catches all edge cases excepct Tales of Ba Sing Se\n",
    "\n",
    "transcript_df = pd.DataFrame()\n",
    "\n",
    "# Loop through each episode\n",
    "# Can expand to more episodes\n",
    "for episode_number, url in enumerate(url_episode_list):\n",
    "    # Splits based on Edge Cases\n",
    "    \n",
    "    data = []  # Initialize data list for the current episode\n",
    "    \n",
    "    #Use if statement to get stuff... \n",
    "    if episode_number not in edge_case_episodes:\n",
    "        \n",
    "        response = requests.get(url)\n",
    "        ep_soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        tables = ep_soup.select(\"table.wikitable\")\n",
    "        print(\"Non Edge Case\")\n",
    "        \n",
    "        desired_table = tables[0]\n",
    "        rows = desired_table.select(\"tbody tr\")\n",
    "\n",
    "    if episode_number in edge_case_episodes:\n",
    "            \n",
    "            if episode_number == 34:\n",
    "                response = requests.get(url)\n",
    "                ep_soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                tables = ep_soup.select(\"table.wikitable\")\n",
    "                \n",
    "                print('Ba Sing Se')\n",
    "                for table in tables[:5]:\n",
    "                    rows = table.select(\"tbody tr\")\n",
    "                    \n",
    "            else:\n",
    "                response = requests.get(url)\n",
    "                ep_soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                tables = ep_soup.select(\"table.wikitable\")\n",
    "                print(\"Edge Case\")\n",
    "\n",
    "                desired_table = tables[1]\n",
    "                rows = desired_table.select(\"tbody tr\")\n",
    "        \n",
    "    split_text = url.split(':', 2)\n",
    "    episode_title = split_text[2]\n",
    "    episode_number += 1\n",
    "    print(episode_number)\n",
    "    print(episode_title)\n",
    "    print(url)\n",
    "    print()\n",
    "\n",
    "    #rows = desired_table.select(\"tbody tr\")\n",
    "\n",
    "    for row in rows:\n",
    "        character_cell = row.find('th')  # Find the <th> for character names\n",
    "        dialogue_cell = row.find('td')   # Find the <td> for character's dialogue\n",
    "\n",
    "        if character_cell and dialogue_cell:  # Ensure both character & dialogue are present\n",
    "            character = character_cell.get_text(strip=True)\n",
    "\n",
    "            # Replace <i> tags with their text (handle italicized text)\n",
    "            for italics in dialogue_cell.find_all('i'):\n",
    "                italic_text_with_suffix = '12345' + italics.get_text(strip=True) + '12345'\n",
    "                italics.replace_with(italic_text_with_suffix)  # Replace <i> tag with modified text\n",
    "\n",
    "            dialogue = dialogue_cell.get_text(strip=True)  # Get the text after modifying italics\n",
    "\n",
    "            # Append data for this row\n",
    "            data.append({'character': character, 'dialogue': dialogue})\n",
    "\n",
    "    # Only create DataFrame if data was extracted        \n",
    "    if data:\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        # Cleaning the dialogue column\n",
    "        df['dialogue'] = df['dialogue'].str.replace(r'\\[.*?\\]', '', regex=True).str.strip()  # Remove text in []\n",
    "        df['dialogue'] = df['dialogue'].str.replace(r'\\(.*?\\)', '', regex=True).str.strip()  # Remove text in ()\n",
    "        df['dialogue'] = df['dialogue'].str.replace('12345', ' ').str.strip()  # Replace '12345' with space\n",
    "\n",
    "        # Add word count\n",
    "        df['word_count'] = df['dialogue'].str.split().apply(len)\n",
    "\n",
    "        # Add episode number and title\n",
    "        df['episode'] = episode_number\n",
    "        df['episode_title'] = episode_title  # Fix the typo\n",
    "\n",
    "        # Concatenate with the main transcript DataFrame\n",
    "        transcript_df = pd.concat([transcript_df, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e191f624-9bbe-477b-9562-2fa7409dcfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Why not pulling in everything?\n",
    "transcript_df.loc[transcript_df['episode']==35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e9a70d-7bd3-4dbf-860a-778dc43e3107",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63827d06-9945-42ed-9d40-cc43c3ba7188",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Catches all edge cases excepct Tales of Ba Sing Se\n",
    "\n",
    "transcript_df = pd.DataFrame()\n",
    "\n",
    "# Loop through each episode\n",
    "# Can expand to more episodes\n",
    "for episode_number, url in enumerate(url_episode_list):\n",
    "    # Splits based on Edge Cases\n",
    "    \n",
    "    data = []  # Initialize data list for the current episode\n",
    "    \n",
    "    #Use if statement to get stuff... \n",
    "    if episode_number not in edge_case_episodes:\n",
    "        \n",
    "        response = requests.get(url)\n",
    "        ep_soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        tables = ep_soup.select(\"table.wikitable\")\n",
    "        print(\"Non Edge Case\")\n",
    "        \n",
    "        desired_table = tables[0]\n",
    "    \n",
    "    if episode_number in edge_case_episodes:\n",
    "        \n",
    "        response = requests.get(url)\n",
    "        ep_soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        tables = ep_soup.select(\"table.wikitable\")\n",
    "        print(\"Edge Case\")\n",
    "        \n",
    "        desired_table = tables[1]\n",
    "    \n",
    "    else:\n",
    "        for table in tables[:5]:\n",
    "            rows = table.select(\"tbody tr\")\n",
    "    \n",
    "    #Ba Sing Se\n",
    "    # Iterate over the first 5 tables (0 to 4)\n",
    "    for table in tables[:5]:  \n",
    "    # Find all rows within each table\n",
    "    \n",
    "        \n",
    "    \n",
    "    split_text = url.split(':', 2)\n",
    "    episode_title = split_text[2]\n",
    "    episode_number += 1\n",
    "    print(episode_number)\n",
    "    print(episode_title)\n",
    "    print(url)\n",
    "    print()\n",
    "\n",
    "    rows = desired_table.select(\"tbody tr\")\n",
    "\n",
    "    for row in rows:\n",
    "        character_cell = row.find('th')  # Find the <th> for character names\n",
    "        dialogue_cell = row.find('td')   # Find the <td> for character's dialogue\n",
    "\n",
    "        if character_cell and dialogue_cell:  # Ensure both character & dialogue are present\n",
    "            character = character_cell.get_text(strip=True)\n",
    "\n",
    "            # Replace <i> tags with their text (handle italicized text)\n",
    "            for italics in dialogue_cell.find_all('i'):\n",
    "                italic_text_with_suffix = '12345' + italics.get_text(strip=True) + '12345'\n",
    "                italics.replace_with(italic_text_with_suffix)  # Replace <i> tag with modified text\n",
    "\n",
    "            dialogue = dialogue_cell.get_text(strip=True)  # Get the text after modifying italics\n",
    "\n",
    "            # Append data for this row\n",
    "            data.append({'character': character, 'dialogue': dialogue})\n",
    "\n",
    "    # Only create DataFrame if data was extracted        \n",
    "    if data:\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        # Cleaning the dialogue column\n",
    "        df['dialogue'] = df['dialogue'].str.replace(r'\\[.*?\\]', '', regex=True).str.strip()  # Remove text in []\n",
    "        df['dialogue'] = df['dialogue'].str.replace(r'\\(.*?\\)', '', regex=True).str.strip()  # Remove text in ()\n",
    "        df['dialogue'] = df['dialogue'].str.replace('12345', ' ').str.strip()  # Replace '12345' with space\n",
    "\n",
    "        # Add word count\n",
    "        df['word_count'] = df['dialogue'].str.split().apply(len)\n",
    "\n",
    "        # Add episode number and title\n",
    "        df['episode'] = episode_number\n",
    "        df['episode_title'] = episode_title  # Fix the typo\n",
    "\n",
    "        # Concatenate with the main transcript DataFrame\n",
    "        transcript_df = pd.concat([transcript_df, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d91c38-ae09-4771-a5f2-0ba709a7b967",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239a7bb4-6bc6-475e-baaa-f442af33dc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7240a7f8-0282-497a-8e36-b4e28ab7ca5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c17f74-3d67-400c-ad0a-92ec4d939de4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d863b26-00af-4f56-9c53-68e79f7c1c73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b005629-90e9-49da-8297-1ace4be13e58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae95937-f4e5-40f8-8be5-5754de0a275c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6172f2cf-710f-4c76-9e9c-dc68b697c40a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ce5c32-4761-456f-8a2e-961e68dccedd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da062df2-7ff2-4800-ba6a-3496a4cb8c27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da30834-4ef1-403e-849c-38c61dc46024",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dec3d5-7e5e-43ec-aa92-3670338a95d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019dcad6-0e07-4666-8d27-2f1099702be6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c9f2ba-192c-47a5-8044-0859722eae7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b61171-da6f-402f-aab9-b6e89dac3820",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "url = 'https://avatar.fandom.com/wiki/Transcript:The_Siege_of_the_North,_Part_2'\n",
    "    \n",
    "response = requests.get(url)\n",
    "ep_soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "tables = ep_soup.select(\"table.wikitable\")\n",
    "\n",
    "print(tables)\n",
    "\n",
    "# data = []\n",
    "\n",
    "# if len(tables) > 1:\n",
    "#     second_table = tables[1]  # Index 1 corresponds to 2nd Table which holds the transcript\n",
    "# # Find all rows within the 2nd Table that holds the transcript\n",
    "#     rows = second_table.select(\"tbody tr\")\n",
    "\n",
    "# for row in rows:\n",
    "#     character_cell = row.find('th')  # Find the <th> for character names\n",
    "#     dialogue_cell = row.find('td')    # Find the <td> for character's dialogue\n",
    "\n",
    "#     if character_cell and dialogue_cell:  # Ensure both character & dialogue are present (thus skipping pure screen directions)\n",
    "#         character = character_cell.get_text(strip=True)\n",
    "\n",
    "#         # Replace <i> tags with their text (ITALICS CAUSING PROBLEMS)\n",
    "#         for italics in dialogue_cell.find_all('i'):\n",
    "#             # Get the text inside <i>, append '123' and replace the <i> with this text\n",
    "#             italic_text_with_suffix =  '12345' + italics.get_text(strip=True) + '12345'\n",
    "#             italics.replace_with(italic_text_with_suffix)  # Replace <i> tag with modified text\n",
    "\n",
    "#         dialogue = dialogue_cell.get_text(strip=True)  # Get the text after modifying italics\n",
    "\n",
    "#         data.append({'character': character, 'dialogue': dialogue})\n",
    "\n",
    "#         df = pd.DataFrame(data)\n",
    "\n",
    "# #Cleaning\n",
    "# df['dialogue'] = df['dialogue'].str.replace(r'\\[.*?\\]', '', regex=True).str.strip()\n",
    "# df['dialogue'] = df['dialogue'].str.replace(r'\\(.*?\\)', '', regex=True).str.strip()\n",
    "# df['dialogue'] = df['dialogue'].str.replace('12345', ' ').str.strip()\n",
    "\n",
    "# #Word Count\n",
    "# df['word_count'] = df['dialogue'].str.split().apply(len)\n",
    "\n",
    "# #Episode \n",
    "# df['episode'] = episode_number\n",
    "# df['espisode_title'] = episode_title \n",
    "            \n",
    "# #     print(episode_title)\n",
    "    \n",
    "# #     transcript_df = pd.concat([transcript_df, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e779a77-6d4f-4619-abe7-3dd46a3d60d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8087fcd0-6d8e-4d17-995f-dea214a85bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try Different Edge Cases\n",
    "    # Previous Problem was based on pulling the wrong table index\n",
    "\n",
    "transcript_df = pd.DataFrame()\n",
    "\n",
    "# Loop through each episode\n",
    "# Can expand to more episodes\n",
    "for episode_number, url in enumerate(url_episode_list):\n",
    "    # Splits based on Edge Cases\n",
    "    if episode_number not in edge_case_episodes:\n",
    "    \n",
    "        split_text = url.split(':', 2)\n",
    "        episode_title = split_text[2]\n",
    "        episode_number += 1\n",
    "        #print(episode_number)\n",
    "        print(episode_title)\n",
    "        print(url)\n",
    "        #print(\"Non-Edge_Case-Episode\")\n",
    "        print()\n",
    "\n",
    "        response = requests.get(url)\n",
    "        ep_soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        tables = ep_soup.select(\"table.wikitable\")\n",
    "\n",
    "        data = []  # Initialize data list for the current episode\n",
    "\n",
    "        #Non Edge Case Data is held in table_index 0\n",
    "        desired_table = tables[0]  # Index 1 corresponds to 2nd Table which holds the transcript\n",
    "        # Find all rows within the 2nd Table that holds the transcript\n",
    "        rows = desired_table.select(\"tbody tr\")\n",
    "\n",
    "        for row in rows:\n",
    "            character_cell = row.find('th')  # Find the <th> for character names\n",
    "            dialogue_cell = row.find('td')   # Find the <td> for character's dialogue\n",
    "\n",
    "            if character_cell and dialogue_cell:  # Ensure both character & dialogue are present\n",
    "                character = character_cell.get_text(strip=True)\n",
    "\n",
    "                # Replace <i> tags with their text (handle italicized text)\n",
    "                for italics in dialogue_cell.find_all('i'):\n",
    "                    italic_text_with_suffix = '12345' + italics.get_text(strip=True) + '12345'\n",
    "                    italics.replace_with(italic_text_with_suffix)  # Replace <i> tag with modified text\n",
    "\n",
    "                dialogue = dialogue_cell.get_text(strip=True)  # Get the text after modifying italics\n",
    "\n",
    "                # Append data for this row\n",
    "                data.append({'character': character, 'dialogue': dialogue})\n",
    "\n",
    "        # Only create DataFrame if data was extracted        \n",
    "        if data:\n",
    "            df = pd.DataFrame(data)\n",
    "\n",
    "            # Cleaning the dialogue column\n",
    "            df['dialogue'] = df['dialogue'].str.replace(r'\\[.*?\\]', '', regex=True).str.strip()  # Remove text in []\n",
    "            df['dialogue'] = df['dialogue'].str.replace(r'\\(.*?\\)', '', regex=True).str.strip()  # Remove text in ()\n",
    "            df['dialogue'] = df['dialogue'].str.replace('12345', ' ').str.strip()  # Replace '12345' with space\n",
    "\n",
    "            # Add word count\n",
    "            df['word_count'] = df['dialogue'].str.split().apply(len)\n",
    "\n",
    "            # Add episode number and title\n",
    "            df['episode'] = episode_number\n",
    "            df['episode_title'] = episode_title  # Fix the typo\n",
    "\n",
    "            # Concatenate with the main transcript DataFrame\n",
    "            transcript_df = pd.concat([transcript_df, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28155551-79d8-4af6-949d-bc471e65df5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d99bda1-bdaa-473b-bc5f-d34fa4afe92e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f390fe23-3c2f-4f62-b0d5-b25eb1558000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Season Loop\n",
    "\n",
    "transcript_df = pd.DataFrame()\n",
    "\n",
    "for episode_number, url in enumerate(url_episode_list):\n",
    "    \n",
    "    split_text = url.split(':', 2)\n",
    "    episode_title = split_text[2]\n",
    "    episode_number += 1\n",
    "    print(episode_number)\n",
    "    print(episode_title)\n",
    "    print(url)\n",
    "    print()\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    ep_soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    tables = ep_soup.select(\"table.wikitable\")\n",
    "\n",
    "    data = []\n",
    "    \n",
    "    if len(tables) > 1:\n",
    "        second_table = tables[1]  # Index 1 corresponds to 2nd Table which holds the transcript\n",
    "    # Find all rows within the 2nd Table that holds the transcript\n",
    "        rows = second_table.select(\"tbody tr\")\n",
    "    \n",
    "    for row in rows:\n",
    "        character_cell = row.find('th')  # Find the <th> for character names\n",
    "        dialogue_cell = row.find('td')    # Find the <td> for character's dialogue\n",
    "    \n",
    "        if character_cell and dialogue_cell:  # Ensure both character & dialogue are present (thus skipping pure screen directions)\n",
    "            character = character_cell.get_text(strip=True)\n",
    "\n",
    "            # Replace <i> tags with their text (ITALICS CAUSING PROBLEMS)\n",
    "            for italics in dialogue_cell.find_all('i'):\n",
    "                # Get the text inside <i>, append '123' and replace the <i> with this text\n",
    "                italic_text_with_suffix =  '12345' + italics.get_text(strip=True) + '12345'\n",
    "                italics.replace_with(italic_text_with_suffix)  # Replace <i> tag with modified text\n",
    "            \n",
    "            dialogue = dialogue_cell.get_text(strip=True)  # Get the text after modifying italics\n",
    "            \n",
    "            data.append({'character': character, 'dialogue': dialogue})\n",
    "\n",
    "            df = pd.DataFrame(data)\n",
    "            \n",
    "    #Cleaning\n",
    "    df['dialogue'] = df['dialogue'].str.replace(r'\\[.*?\\]', '', regex=True).str.strip()\n",
    "    df['dialogue'] = df['dialogue'].str.replace(r'\\(.*?\\)', '', regex=True).str.strip()\n",
    "    df['dialogue'] = df['dialogue'].str.replace('12345', ' ').str.strip()\n",
    "\n",
    "    #Word Count\n",
    "    df['word_count'] = df['dialogue'].str.split().apply(len)\n",
    "\n",
    "    #Episode \n",
    "    df['episode'] = episode_number\n",
    "    df['espisode_title'] = episode_title \n",
    "            \n",
    "#     print(episode_title)\n",
    "    \n",
    "#     transcript_df = pd.concat([transcript_df, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91afece-201d-46f3-9e0d-2e9eb54dea3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa6c73c-9017-4fbe-8a89-646a5fc33c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31614de-9b95-49fe-9209-b5ffa0fe7248",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80d129f-a9bb-4fa5-a0a8-b4abf57fe177",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = transcript_df[transcript_df['episode'] == 7]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110be336-419d-4eb9-99d5-99331c8051eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "#pd.set_option('display.max_colwidth', None) \n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "for episode in range(1,21):\n",
    "    test = transcript_df[transcript_df['episode'] == episode]\n",
    "    print(test)\n",
    "    #print(tabulate(test, headers='keys', tablefmt='pretty'))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533b524c-320e-4e08-a598-efcb01c52ac6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transcript_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c910729-7174-4944-b329-b3ef5c21d0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_df.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49468849-d4e6-4676-8d13-6fdf0dc78237",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First Episode\n",
    "episode_base = url_episode_list[0]\n",
    "response = requests.get(episode_base)\n",
    "ep_soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "#tables = ep_soup.find_all(\"table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4987e4-b945-4478-b4af-9e45be5897ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = ep_soup.select(\"table.wikitable\")\n",
    "data = []\n",
    "\n",
    "# Step 4: Check if there are at least 2 tables and select the second one\n",
    "if len(tables) > 1:\n",
    "    second_table = tables[1]  # Index 1 corresponds to the second table\n",
    "    # Now, find all rows within this second table's tbody\n",
    "    rows = second_table.select(\"tbody tr\")\n",
    "    \n",
    "    for row in rows:\n",
    "        character_cell = row.find('th')  # Find the <th> for character names\n",
    "        dialogue_cell = row.find('td')    # Find the <td> for dialogue\n",
    "    \n",
    "        if character_cell and dialogue_cell:  # Ensure both are present\n",
    "            character = character_cell.get_text(strip=True)\n",
    "\n",
    "            # Replace <i> tags with their text (ITALICS CAUSING PROBLEMS)\n",
    "            for italics in dialogue_cell.find_all('i'):\n",
    "                # Get the text inside <i>, append '123' and replace the <i> with this text\n",
    "                italic_text_with_suffix =  '12345' + italics.get_text(strip=True) + '12345'\n",
    "                italics.replace_with(italic_text_with_suffix)  # Replace <i> tag with modified text\n",
    "            \n",
    "            dialogue = dialogue_cell.get_text(strip=True)  # Get the text after modifying italics\n",
    "            \n",
    "            data.append({'character': character, 'dialogue': dialogue})\n",
    "\n",
    "            df = pd.DataFrame(data)\n",
    "            \n",
    "            #Cleaning\n",
    "            df['dialogue'] = df['dialogue'].str.replace(r'\\[.*?\\]', '', regex=True).str.strip()\n",
    "            df['dialogue'] = df['dialogue'].str.replace(r'\\(.*?\\)', '', regex=True).str.strip()\n",
    "            df['dialogue'] = df['dialogue'].str.replace('12345', ' ').str.strip()\n",
    "            \n",
    "            #Word Count\n",
    "            df['word_count'] = df['dialogue'].str.split().apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041e46a1-d94d-4469-9fa9-e04fa04d7afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None) \n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcdb416-7d62-41cf-a685-74ab05e1bd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None) \n",
    "df.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a314da-02cb-4a1f-9c1b-ded611f93b52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Table 1 - Intro\n",
    "if len(tables) > 0:\n",
    "    print(\"Second table:\")\n",
    "    print()\n",
    "    print(tables[1].prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ae857b-b83f-4a21-8b19-746dd703382e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b1a29a-63e6-401d-9fd2-6a529d2d48e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a4dcc9-6ac5-46b1-98b5-04085559dac9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
